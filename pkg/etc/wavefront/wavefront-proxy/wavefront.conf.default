#
# Wavefront proxy configuration file
#
#   Typically in /etc/wavefront/wavefront-proxy/wavefront.conf
#
########################################################################################################################
# Wavefront API endpoint URL. Usually the same as the URL of your Wavefront instance, with an `api`
# suffix -- or Wavefront provides the URL.
server=https://try.wavefront.com/api/

# The hostname will be used to identify the internal proxy statistics around point rates, JVM info, etc.
# We strongly recommend setting this to a name that is unique among your entire infrastructure, to make this
# proxy easy to identify. This hostname does not need to correspond to any actual hostname or DNS entry; it's merely
# a string that we pass with the internal stats and ~proxy.* metrics.
#hostname=my.proxy.host.com

# The Token is any valid API token for an account that has *Proxy Management* permissions. To get to the token:
# 1. Click the gear icon at the top right in the Wavefront UI.
# 2. Click your account name (usually your email)
# 3. Click *API access*.
#token=

####################################################### INPUTS #########################################################
# Comma-separated list of ports to listen on for Wavefront formatted data (Default: 2878)
pushListenerPorts=2878
## Maximum line length for received points in plaintext format on Wavefront/OpenTSDB/Graphite ports. Default: 32KB
#pushListenerMaxReceivedLength=32768
## Maximum request size (in bytes) for incoming HTTP requests on Wavefront/OpenTSDB/Graphite ports. Default: 16MB
#pushListenerHttpBufferSize=16777216

## Delta counter pre-aggregation settings.
## Comma-separated list of ports to listen on for Wavefront-formatted delta counters. Helps reduce
## outbound point rate by pre-aggregating delta counters at proxy. Default: none
#deltaCountersAggregationListenerPorts=12878
## Interval between flushing aggregating delta counters to Wavefront. Defaults to 30 seconds.
#deltaCountersAggregationIntervalSeconds=60

## Graphite input settings.
## If you enable either `graphitePorts` or `picklePorts`, make sure to uncomment and set `graphiteFormat` as well.
## Comma-separated list of ports to listen on for collectd/Graphite formatted data (Default: none)
#graphitePorts=2003
## Comma-separated list of ports to listen on for Graphite pickle formatted data (from carbon-relay) (Default: none)
#picklePorts=2004
## Which fields (1-based) should we extract and concatenate (with dots) as the hostname?
#graphiteFormat=2
## Which characters should be replaced by dots in the hostname, after extraction? (Default: _)
#graphiteDelimiters=_
## Comma-separated list of fields (metric segments) to remove (1-based). This is an optional setting. (Default: none)
#graphiteFieldsToRemove=3,4,5

## DDI/Relay endpoint: in environments where no direct outbound connections to Wavefront servers are possible, you can
## use another Wavefront proxy that has outbound access to act as a relay and forward all the data received on that
## endpoint (from direct data ingestion clients and/or other proxies) to Wavefront servers.
## This setting is a comma-separated list of ports. (Default: none)
#pushRelayListenerPorts=2978
## If true, aggregate histogram distributions received on the relay port.
## Please refer to histogram settings section below for more configuration options. Default: false
#pushRelayHistogramAggregator=false

## Comma-separated list of ports to listen on for OpenTSDB formatted data (Default: none)
#opentsdbPorts=4242

## Comma-separated list of ports to listen on for HTTP JSON formatted data (Default: none)
#jsonListenerPorts=3878

## Comma-separated list of ports to listen on for HTTP collectd write_http data (Default: none)
#writeHttpJsonListenerPorts=4878

################################################# DATA PREPROCESSING ###################################################
## Path to the optional config file with preprocessor rules (advanced regEx replacements and allow/block lists)
#preprocessorConfigFile=/etc/wavefront/wavefront-proxy/preprocessor_rules.yaml

# When using the Wavefront or TSDB data formats, the proxy will automatically look for a tag named
# source= or host= (preferring source=) and treat that as the source/host within Wavefront.
# customSourceTags is a comma-separated, ordered list of additional tag keys to use if neither
# source= or host= is present
customSourceTags=fqdn, hostname

## The prefix should either be left undefined, or can be any  prefix you want
## prepended to all data points coming through this proxy (such as `production`).
#prefix=production

## Regex pattern (Java) that input lines must match to be accepted. Use preprocessor rules for finer control.
#allow=^(production|stage).*
## Regex pattern (Java) that input lines must NOT match to be accepted. Use preprocessor rules for finer control.
#block=^(qa|development|test).*

## This setting defines the cut-off point for what is considered a valid timestamp for back-dated points.
## Default (and recommended) value is 8760 (1 year), so all the data points from more than 1 year ago will be rejected.
#dataBackfillCutoffHours=8760
## This setting defines the cut-off point for what is considered a valid timestamp for pre-dated points.
## Default (and recommended) value is 24 (1 day), so all the data points from more than 1 day in future will be rejected.
#dataPrefillCutoffHours=24

################################################## ADVANCED SETTINGS ###################################################
## Number of threads that flush data to the server. If not defined in wavefront.conf it defaults to the
## number of processors (min 4). Setting this value too large will result in sending batches that are
## too small to the server and wasting connections. This setting is per listening port.
#flushThreads=4
## Max points per single flush. Default: 40000.
#pushFlushMaxPoints=40000
## Max histograms per single flush. Default: 10000.
#pushFlushMaxHistograms=10000
## Max spans per single flush. Default: 5000.
#pushFlushMaxSpans=5000
## Max span logs per single flush. Default: 1000.
#pushFlushMaxSpanLogs=1000

## Milliseconds between flushes to the Wavefront servers. Typically 1000.
#pushFlushInterval=1000

## Limit outbound points per second rate at the proxy. Default: do not throttle
#pushRateLimit=20000
## Limit outbound histograms per second rate at the proxy. Default: do not throttle
#pushRateLimitHistograms=2000
## Limit outbound spans per second rate at the proxy. Default: do not throttle
#pushRateLimitSpans=1000
## Limit outbound span logs per second rate at the proxy. Default: do not throttle
#pushRateLimitSpanLogs=1000

## Max number of burst seconds to allow when rate limiting to smooth out uneven traffic.
## Set to 1 when doing data backfills. Default: 10
#pushRateLimitMaxBurstSeconds=10

## Number of threads retrying failed transmissions. Defaults to the number of processors (min. 4)
## Buffer files are maxed out at 2G each so increasing the number of retry threads effectively governs
## the maximum amount of space the proxy will use to buffer points locally
#retryThreads=4
## Location of buffer.* files for saving failed transmissions for retry. Default: /var/spool/wavefront-proxy/buffer
#buffer=/var/spool/wavefront-proxy/buffer
## For exponential backoff when retry threads are throttled, the base (a in a^b) in seconds.  Default 2.0
#retryBackoffBaseSeconds=2.0
## Whether to split the push batch size when the push is rejected by Wavefront due to rate limit.  Default false.
#splitPushWhenRateLimited=false

## The following settings are used to connect to Wavefront servers through a HTTP proxy:
#proxyHost=localhost
#proxyPort=8080
## Optional: if http proxy requires authentication
#proxyUser=proxy_user
#proxyPassword=proxy_password
## HTTP proxies may implement a security policy to only allow traffic with particular User-Agent header values.
## When set, overrides User-Agent in request headers for outbound HTTP requests.
#httpUserAgent=WavefrontProxy

## HTTP client settings
## Control whether metrics traffic from the proxy to the Wavefront endpoint is gzip-compressed. Default: true
#gzipCompression=true
## If gzipCompression is enabled, sets compression level (1-9). Higher compression levels slightly reduce
## the volume of traffic between the proxy and Wavefront, but use more CPU. Default: 4
#gzipCompressionLevel=4
## Connect timeout (in milliseconds). Default: 5000 (5s)
#httpConnectTimeout=5000
## Request timeout (in milliseconds). Default: 10000 (10s)
#httpRequestTimeout=10000
## Max number of total connections to keep open (Default: 200)
#httpMaxConnTotal=100
## Max connections per route to keep open (Default: 100)
#httpMaxConnPerRoute=100

## Close idle inbound connections after specified time in seconds. Default: 300 (5 minutes)
#listenerIdleConnectionTimeout=300

## The following setting enables SO_LINGER on listening ports with the specified linger time in seconds (Default: off)
#soLingerTime=0

## Max number of points that can stay in memory buffers before spooling to disk. Defaults to 16 * pushFlushMaxPoints,
## minimum allowed size: pushFlushMaxPoints. Setting this value lower than default reduces memory usage but will force
## the proxy to spool to disk more frequently if you have points arriving at the proxy in short bursts.
#pushMemoryBufferLimit=640000

## If there are blocked points, a small sampling of them can be written into the main log file.
## This setting how many lines to print to the log every 10 seconds. Typically 5.
#pushBlockedSamples=5
## When logging blocked points is enabled (see https://docs.wavefront.com/proxies_configuring.html#blocked-point-log
## for more details), by default all blocked points/histograms/spans etc are logged into the same `RawBlockedPoints`
## logger. Settings below allow finer control over these loggers:
## Logger name for blocked points. Default: RawBlockedPoints.
#blockedPointsLoggerName=RawBlockedPoints
## Logger name for blocked histograms. Default: RawBlockedPoints.
#blockedHistogramsLoggerName=RawBlockedPoints
## Logger name for blocked spans. Default: RawBlockedPoints.
#blockedSpansLoggerName=RawBlockedPoints

## Settings for incoming HTTP request authentication. Authentication is done by a token, proxy is looking for
## tokens in the querystring ("token=" and "api_key=" parameters) and in request headers ("X-AUTH-TOKEN: ",
## "Authorization: Bearer", "Authorization: " headers). TCP streams are disabled when authentication is turned on.
## Allowed authentication methods: NONE, STATIC_TOKEN, HTTP_GET, OAUTH2. Default: NONE
## - NONE: All requests are considered valid
## - STATIC_TOKEN: Compare incoming token with the value of authStaticToken setting.
## - OAUTH2: Validate all tokens against a RFC7662-compliant token introspection endpoint.
## - HTTP_GET: Validate all tokens against a specific URL. Use {{token}} placeholder in the URL to pass the token
##             in question to the endpoint. Use of https is strongly recommended for security reasons. The endpoint
##             must return any 2xx status for valid tokens, any other response code is considered a fail.
#authMethod=NONE
## URL for the token introspection endpoint used to validate tokens for incoming HTTP requests.
## Required when authMethod is OAUTH2 or HTTP_GET
#authTokenIntrospectionServiceUrl=https://auth.acme.corp/api/token/{{token}}/validate
## Optional credentials for use with the token introspection endpoint if it requires authentication.
#authTokenIntrospectionAuthorizationHeader=Authorization: Bearer <token>
## Cache TTL (in seconds) for token validation results (re-authenticate when expired). Default: 600 seconds
#authResponseRefreshInterval=600
## Maximum allowed cache TTL (in seconds) for token validation results when token introspection service is
## unavailable. Default: 86400 seconds (1 day)
#authResponseMaxTtl=86400
## Static token that is considered valid for all incoming HTTP requests. Required when authMethod = STATIC_TOKEN.
#authStaticToken=token1234abcd

############################################# LOGS TO METRICS SETTINGS #################################################
## Port on which to listen for FileBeat data (Lumberjack protocol). Default: none
#filebeatPort=5044
## Port on which to listen for raw logs data (TCP and HTTP). Default: none
#rawLogsPort=5045
## Maximum line length for received raw logs (Default: 4096)
#rawLogsMaxReceivedLength=4096
## Maximum allowed request size (in bytes) for incoming HTTP requests with raw logs (Default: 16MB)
#rawLogsHttpBufferSize=16777216
## Location of the `logsingestion.yaml` configuration file
#logsIngestionConfigFile=/etc/wavefront/wavefront-proxy/logsingestion.yaml

########################################### DISTRIBUTED TRACING SETTINGS ###############################################
## Comma-separated list of ports to listen on for spans in Wavefront format. Defaults to none.
#traceListenerPorts=30000
## Maximum line length for received spans and span logs (Default: 1MB)
#traceListenerMaxReceivedLength=1048576
## Maximum allowed request size (in bytes) for incoming HTTP requests on tracing ports (Default: 16MB)
#traceListenerHttpBufferSize=16777216

## Comma-separated list of ports to listen on for spans from SDKs that send raw data. Unlike
## `traceListenerPorts` setting, also derives RED metrics based on received spans. Defaults to none.
#customTracingListenerPorts=30001
## Custom application name for spans received on customTracingListenerPorts. Defaults to defaultApp.
#customTracingApplicationName=defaultApp
## Custom service name for spans received on customTracingListenerPorts. Defaults to defaultService.
#customTracingServiceName=defaultService

## Comma-separated list of ports on which to listen on for Jaeger Thrift formatted data. Defaults to none.
#traceJaegerListenerPorts=14267
## Custom application name for traces received on Jaeger's traceJaegerListenerPorts.
#traceJaegerApplicationName=Jaeger
## Comma-separated list of ports on which to listen on for Zipkin trace data over HTTP. Defaults to none.
## Recommended value is 9411, which is the port Zipkin's server listens on and is the default configuration in Istio.
#traceZipkinListenerPorts=9411
## Custom application name for traces received on Zipkin's traceZipkinListenerPorts.
#traceZipkinApplicationName=Zipkin
## Comma-separated list of additional custom tag keys to include along as metric tags for the derived
## RED (Request, Error, Duration) metrics. Applicable to Jaeger and Zipkin integration only.
#traceDerivedCustomTagKeys=tenant, env, location

## The following settings are used to configure trace data sampling:
## The rate for traces to be sampled. Can be from 0.0 to 1.0. Defaults to 1.0
#traceSamplingRate=1.0
## The minimum duration threshold (in milliseconds) for the spans to be sampled.
## Spans above the given duration are reported. Defaults to 0 (include everything).
#traceSamplingDuration=0
## Always sample spans with an error tag (set to true) ignoring other sampling configuration. Defaults to true.
#traceAlwaysSampleErrors=false

########################################## HISTOGRAM ACCUMULATION SETTINGS #############################################
## Histograms can be ingested in Wavefront scalar and distribution format. For scalar samples ports can be specified for
## minute, hour and day granularity. Granularity for the distribution format is encoded inline. Before using any of
## these settings, reach out to Wavefront Support to ensure your account is enabled for native Histogram support and
## to optimize the settings for your specific use case.

## Accumulation parameters
## Directory for persisting proxy state, must be writable.
#histogramStateDirectory=/var/spool/wavefront-proxy
## Interval to write back accumulation changes to disk in milliseconds (only applicable when memory cache is enabled).
#histogramAccumulatorResolveInterval=5000
## Interval to check for histograms ready to be sent to Wavefront, in milliseconds.
#histogramAccumulatorFlushInterval=10000
## Max number of histograms to send to Wavefront in one flush (Default: no limit)
#histogramAccumulatorFlushMaxBatchSize=4000
## Maximum line length for received histogram data (Default: 65536)
#histogramMaxReceivedLength=65536
## Maximum allowed request size (in bytes) for incoming HTTP requests on histogram ports (Default: 16MB)
#histogramHttpBufferSize=16777216

## Wavefront format, minute aggregation:
## Comma-separated list of ports to listen on.
#histogramMinuteListenerPorts=40001
## Time-to-live in seconds for a minute granularity accumulation on the proxy (before the intermediary is shipped to WF).
#histogramMinuteFlushSecs=70
## Bounds the number of centroids per histogram. Must be between 20 and 1000, default: 32
#histogramMinuteCompression=32
## Expected upper bound of concurrent accumulations. Should be approximately the number of timeseries * 2 (use a higher
## multiplier if out-of-order points more than 1 minute apart are expected). Setting this value too high will cause
## excessive disk space usage, setting this value too low may cause severe performance issues.
#histogramMinuteAccumulatorSize=1000
## Enabling memory cache reduces I/O load with fewer time series and higher frequency data (more than 1 point per
## second per time series). Default: false
#histogramMinuteMemoryCache=false
## Whether to persist accumulation state. When true, all histograms are written to disk immediately if memory cache is
## disabled, or every histogramAccumulatorResolveInterval seconds if memory cache is enabled.
## If accumulator is not persisted, up to histogramMinuteFlushSecs seconds worth of histograms may be lost on proxy shutdown.
#histogramMinuteAccumulatorPersisted=false

## Wavefront format, hour aggregation:
## Comma-separated list of ports to listen on.
#histogramHourListenerPorts=40002
## Time-to-live in seconds for an hour granularity accumulation on the proxy (before the intermediary is shipped to WF).
#histogramHourFlushSecs=4200
## Bounds the number of centroids per histogram. Must be between 20 and 1000, default: 32
#histogramHourCompression=32
## Expected upper bound of concurrent accumulations. Should be approximately the number of timeseries * 2 (use a higher
## multiplier if out-of-order points more than 1 hour apart are expected). Setting this value too high will cause
## excessive disk space usage, setting this value too low may cause severe performance issues.
#histogramHourAccumulatorSize=100000
## Enabling memory cache reduces I/O load with fewer time series and higher frequency data (more than 1 point per
## second per time series). Default: false
#histogramHourMemoryCache=true
## Whether to persist accumulation state. When true, all histograms are written to disk immediately if memory cache is
## disabled, or every `histogramAccumulatorResolveInterval` seconds if memory cache is enabled.
## If accumulator is not persisted, up to `histogramHourFlushSecs` seconds worth of histograms may be lost on proxy shutdown.
#histogramHourAccumulatorPersisted=true

## Wavefront format, day aggregation:
## Comma-separated list of ports to listen on.
#histogramDayListenerPorts=40003
## Time-to-live in seconds for a day granularity accumulation on the proxy (before the intermediary is shipped to WF).
#histogramDayFlushSecs=18000
## Bounds the number of centroids per histogram. Must be between 20 and 1000, default: 32
#histogramDayCompression=32
## Expected upper bound of concurrent accumulations. Should be approximately the number of timeseries * 2 (use a higher
## multiplier if out-of-order points more than 1 day apart are expected). Setting this value too high will cause
## excessive disk space usage, setting this value too low may cause severe performance issues.
#histogramDayAccumulatorSize=100000
## Enabling memory cache reduces I/O load with fewer time series and higher frequency data (more than 1 point per
## second per time series). Default: false
#histogramDayMemoryCache=false
## Whether to persist accumulation state. When true, all histograms are written to disk immediately if memory cache is
## disabled, or every `histogramAccumulatorResolveInterval` seconds if memory cache is enabled.
## If accumulator is not persisted, up to `histogramDayFlushSecs` seconds worth of histograms may be lost on proxy shutdown.
#histogramDayAccumulatorPersisted=true

## Distribution format:
## Comma-separated list of ports to listen on.
#histogramDistListenerPorts=40000
## Time-to-live in seconds for a distribution accumulation on the proxy (before the intermediary is shipped to WF).
#histogramDistFlushSecs=70
## Bounds the number of centroids per histogram. Must be between 20 and 1000, default: 32
#histogramDistCompression=32
## Expected upper bound of concurrent accumulations. Should be approximately the number of timeseries * 2 (use a higher
## multiplier if out-of-order points more than 1 bin apart are expected). Setting this value too high will cause
## excessive disk space usage, setting this value too low may cause severe performance issues.
#histogramDistAccumulatorSize=100000
## Enabling memory cache reduces I/O load with fewer time series and higher frequency data (more than 1 point per
## second per time series). Default: false
#histogramDistMemoryCache=false
## Whether to persist accumulation state. When true, all histograms are written to disk immediately if memory cache is
## disabled, or every histogramAccumulatorResolveInterval seconds if memory cache is enabled.
## If accumulator is not persisted, up to histogramDistFlushSecs seconds worth of histograms may be lost on proxy shutdown.
#histogramDistAccumulatorPersisted=true

## Histogram accumulation for relay ports (only applicable if pushRelayHistogramAggregator is true):
## Time-to-live in seconds for the accumulator (before the intermediary is shipped to WF). Default: 70
#pushRelayHistogramAggregatorFlushSecs=70
## Bounds the number of centroids per histogram. Must be between 20 and 1000, default: 32
#pushRelayHistogramAggregatorCompression=32
## Expected upper bound of concurrent accumulations. Should be approximately the number of timeseries * 2 (use a higher
## multiplier if out-of-order points more than 1 bin apart are expected). Setting this value too high will cause
## excessive disk space usage, setting this value too low may cause severe performance issues.
#pushRelayHistogramAggregatorAccumulatorSize=1000000
